# names in parentheses are special, all other names are not!
# (e.g. add/extend this with any custom fields)
(project):
    # the local_data file will be auto-generated
    # (its for machine-specific data)
    # so probably git-ignore whatever path you pick
    (local_data): ./local_data.ignore.yaml
    
    (path_to):
        results_folder: "./results"
        recent_results: "./results/recent_results.csv"
        transfer_autoencoder_results: ./results/transfer_autoencoder_results.csv
        huffman_coder: "./huffman_code.ignore.json"
        negative_examples: ./negative_examples.json
        positive_examples: ./positive_examples.json
        negative_examples_genes: ./negative_examples_genes.json
        positive_examples_genes: ./positive_examples_genes.json
        all_negative_examples: ./data/all_negative_examples.ignore.json
        all_positive_examples: ./data/all_positive_examples.ignore.json
        all_negative_examples_genes: ./data/all_negative_examples_genes.ignore.json
        all_positive_examples_genes: ./data/all_positive_examples_genes.ignore.json
        important_features_image: ./figures/recent_feature_importance.png
        prev_parameters: ./prev_parameters.json
        autoencoder_checker_cache: ./cache.ignore/autoencoder_parameter_check.json
        prev_autoencoder: ./cache.ignore/autoencoder.pickle
        autoencoder_losses: ./results/autoencoder_losses.csv
    
    todo:
        - dont add plant features
        - do autoencoder:
            - how many layers? 50 units, 20 units
            - latent space: 2 latent, 5, 10,
            - Relu
        - maybe switch to protien level prediction:
            - maybe evaluate if 
            - maybe use HMM's
            - LSTM
    # example profiles
    (profiles):
        (default):
            feature_set: "huffman_negative"
            negative_label: -1
            positive_label: 1
            gene_classification_threshold: 3
            generate_data:
                windowPadding: 10 # + or - 10 amino acids
                aminoMatchPattern: "S"
                useHuffmanEncoding: false
                positionInvariantFarAwayValue: 30
                huffmanEncoderCap: 30
                minOneSideEncodedLength: 5
                datasetSize: 50_000
                preprocessing:
                    shouldUseSimplifier: false
                featureToInclude:
                    normalPositionalData: true
                    positionInvariantHuffman: false
                    positionInvariantPhysicochemicalCategories: false
                    physicochemicalCategories: false
                    handCodedMotifs: false
            
            classifier_truncate_sample: 50_000
            phos_classifier_hyperparameters:
                number_of_layers: 1 # layers in additon to the encoder layers
                final_activation_function_eval: "lambda x: x/(1 + torch.abs(x))" # modified sigmoid
                activation_function_eval:       "lambda x: x/(1 + torch.abs(x))" # modified sigmoid
                loss_function_eval: "lambda a,b: F.binary_cross_entropy((a+1)/2, (b+1)/2)" # still binary_cross_entropy, just works for 1 to -1 instead of 1 to 0
                # final_activation_function_eval: "nn.Sigmoid()"
                # activation_function_eval:       "nn.Sigmoid()"
                # loss_function_eval: "F.binary_cross_entropy"
                learning_rate: 0.01
                momentum: 0.5
                
            autoencoder_hyperparameters:
                max_epochs: 400
                batch_size: 1024
                latent_size: 128
                number_of_layers: 3
                learning_rate: 0.20
                momentum: 0.5
                activation_function_eval: "nn.Sigmoid()"
                loss_function_eval: "lambda *args: F.binary_cross_entropy(torch.clip(args[0], min=0, max=1), torch.clip(args[1], min=0, max=1))" # TODO: figure out why there is one random 1.251 value in the tensor that causes binary_cross_entropy to fail
                validation_threshold: 0.02
                number_of_folds: 3

