# names in parentheses are special, all other names are not!
# (e.g. add/extend this with any custom fields)
(project):
    # the local_data file will be auto-generated
    # (its for machine-specific data)
    # so probably git-ignore whatever path you pick
    (local_data): ./local_data.ignore.yaml
    
    (path_to):
        recent_results: "./recent_results.csv"
        huffman_coder: "./huffman_code.ignore.json"
        negative_examples: ./negative_examples.json
        positive_examples: ./positive_examples.json
        important_features_image: ./figures/recent_feature_importance.png
        prev_parameters: ./prev_parameters.json
        autoencoder_checker_cache: ./cache.ignore/autoencoder_parameter_check.json
        prev_autoencoder: ./cache.ignore/autoencoder.pickle
    
    todo:
        - dont add plant features
        - do autoencoder:
            - how many layers? 50 units, 20 units
            - latent space: 2 latent, 5, 10,
            - Relu
        - maybe switch to protien level prediction
            - maybe evaluate if 
            - maybe use HMM's
            - LSTM
    # example profiles
    (profiles):
        (default):
            feature_set: "huffman_negative"
            generate_data:
                windowPadding: 10 # + or - 10 amino acids
                aminoMatchPattern: "S"
                useHuffmanEncoding: true
                positionInvariantFarAwayValue: 30
                huffmanEncoderCap: 30
                minOneSideEncodedLength: 5
                datasetSize: 20_000
                preprocessing:
                    shouldUseSimplifier: false
                featureToInclude:
                    normalPositionalData: true
                    positionInvariantHuffman: true
                    positionInvariantPhysicochemicalCategories: false
                    physicochemicalCategories: false
                    handCodedMotifs: false
            
            phos_classifier_hyperparameters:
                number_of_layers: 1 # layers in additon to the encoder layers
                final_activation_function_eval: "lambda x: x/(1 + torch.abs(x))" # modified sigmoid
                activation_function_eval:       "lambda x: x/(1 + torch.abs(x))" # modified sigmoid
                loss_function_eval: "lambda a,b: F.binary_cross_entropy((a+1)/2, (b+1)/2)" # still binary_cross_entropy, just works for 1 to -1 instead of 1 to 0
                # final_activation_function_eval: "nn.Sigmoid()"
                # activation_function_eval:       "nn.Sigmoid()"
                # loss_function_eval: "F.binary_cross_entropy"
                learning_rate: 0.01
                momentum: 0.5
                
            autoencoder_hyperparameters:
                max_epochs: 1
                batch_size: 64
                latent_size: 30
                number_of_layers: 3
                learning_rate: 0.01
                momentum: 0.5
                activation_function_eval: "nn.ReLU()"
                loss_function_eval: "F.mse_loss"

